# Import Liabraries and Load data
import pandas as pd 
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, r2_score, roc_auc_score, classification_report
import pickle 
df = pd.read_csv('Phishing_Websites_Data.csv')
df.head()
# Basic EDA
df.describe()
df.shape
df.isnull().sum()
df = df.drop_duplicates()
df.columns
df.dtypes
Note: The all columns are already in numeric so there is no need for encoding.
# Split Data
X = df.drop('Result', axis=1)
y = df['Result']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
# Train Model
Random Forest Classifier model
rf_model = RandomForestClassifier()
rf_model.fit(X_train_scaled, y_train)

y_pred = rf_model.predict(X_test_scaled)
#check model performance
print("accuracy_score", accuracy_score(y_test, y_pred))
print("confusion_matrix", confusion_matrix(y_test, y_pred))
print("r2_score", r2_score(y_test, y_pred))
print("classification_report", classification_report(y_test, y_pred))
print("roc_auc_score:", roc_auc_score(y_test, y_pred))
XGBOOST model
import numpy as np

# Convert -1 to 0
y_train_fixed = np.where(y_train == -1, 0, y_train)
y_test_fixed = np.where(y_test == -1, 0, y_test)

xg_model = XGBClassifier()
xg_model.fit(X_train_scaled, y_train_fixed)

pred = xg_model.predict(X_test_scaled)

print("Accuracy :", accuracy_score(y_test_fixed, pred))
print(classification_report(y_test_fixed, pred))
print("roc_auc_score:", roc_auc_score(y_test_fixed, pred))
Both model give accuracy more than 95percent but XGBOOST gives more acurracy so we choose that model for a trian and predict the result. 
# Visualization
# Compute confusion matrix
cm = confusion_matrix(y_test_fixed, pred)
# Plot
plt.figure(figsize=(6,4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix - XGBoost")
plt.show()

#pie chart of target labels:
labels = ["Class 0", "Class 1"]
values = [sum(y_train_fixed==0), sum(y_train_fixed==1)]
plt.figure(figsize=(6,6))
plt.pie(values, labels=labels, autopct="%1.1f%%", startangle=90)
plt.title("Target Variable Distribution")
plt.show()

# Save XGBOOST Model & Scaler For Deployment
pickle.dump(xg_model, open("model.pkl", "wb"))
pickle.dump(scaler, open("scaler.pkl", "wb"))
